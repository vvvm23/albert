# ALBERT (A Lite BERT)
PyTorch Implementation of "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"

Original paper can be found [here](https://arxiv.org/abs/1909.11942).

This repository will contain a PyTorch implementation of ALBERT and various wrappers around it for pre-training and fine-tuning tasks.

## Repository Structure
`TODO: Detail on structure of this repository`

## Usage
`TODO: Usage Instructions`

### Data Preparation
`TBA`

### Configuration
`TBA`

### Model Pre-training
`TBA`

### Model Fine-tuning
`TBA`

### Inference
`TBA`

## Modifications
`TODO: Detail deviations from original work`

## Checkpoints
`TODO: Add model checkpoints`

---

### Roadmap
- [X] ALBERT Core
- [X] BERT alternative option
- [X] Pretraining Wrappers
- [ ] Finetuning Wrappers
- [ ] Preprocessing Pipeline
    - [ ] Native (Slower)
    - [ ] ðŸ¤—Version (Faster)
- [ ] Training Scripts
- [ ] Inference Scripts
- [ ] More attention approximation options
- [ ] Fancy Logging
- [ ] Automatic Mixed-Precision Operations
- [ ] Distributed Training

### Citations
**ALBERT: A Lite BERT for Self-supervised Learning of Language Representations**
```
@misc{lan2020albert,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
